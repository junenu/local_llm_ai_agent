import streamlit as st
from datetime import datetime, timedelta
from typing import List

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langchain_ollama import ChatOllama
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict


# æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
class ConversationManager:
    def __init__(self, timeout_seconds: int = 300):
        self.timeout = timedelta(seconds=timeout_seconds)
        self._history: List[BaseMessage] = []
        self._last_updated: datetime = datetime.now()

    def _check_timeout(self):
        if datetime.now() - self._last_updated > self.timeout:
            self._history = []

    def get_history(self) -> List[BaseMessage]:
        self._check_timeout()
        return self._history

    def add_conversation_pair(self, user_message: HumanMessage, ai_message: AIMessage):
        self._history.extend([user_message, ai_message])
        self._last_updated = datetime.now()

    def clear_history(self):
        self._history = []
        self._last_updated = datetime.now()


class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]


@st.cache_resource
def initialize_llm_and_graph():
    llm = ChatOllama(model="gemma3:12b")

    def call_llm(state: AgentState) -> AgentState:
        resp = llm.invoke(state["messages"])
        return {"messages": [resp]}

    graph = StateGraph(AgentState)
    graph.add_node("llm", call_llm)
    graph.set_entry_point("llm")
    graph.add_edge("llm", END)
    app = graph.compile()

    return app


def main():
    st.set_page_config(page_title="Local LLM Chat", page_icon="ğŸ¤–", layout="wide")

    st.title("ğŸ¤– Local LLM Chat")
    st.markdown("Gemma3:12Bã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«AIãƒãƒ£ãƒƒãƒˆ")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "conversation_manager" not in st.session_state:
        st.session_state.conversation_manager = ConversationManager(timeout_seconds=300)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # LLMã¨Graphã®åˆæœŸåŒ–
    app = initialize_llm_and_graph()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("è¨­å®š")

        if st.button("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", type="secondary"):
            st.session_state.conversation_manager.clear_history()
            st.session_state.messages = []
            st.rerun()

        st.markdown("---")
        st.markdown("### ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«")
        st.info("Gemma3:12B (Ollama)")

        st.markdown("### ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        st.info("5åˆ†é–“")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            message_placeholder = st.empty()

            # LangChainãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            user_message = HumanMessage(content=prompt)
            current_history = st.session_state.conversation_manager.get_history()
            state = {"messages": current_history + [user_message]}

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†
            full_response = ""

            try:
                for chunk in app.stream(state):
                    if "llm" in chunk:
                        ai_message_chunk = chunk["llm"]["messages"][-1]
                        if ai_message_chunk.content:
                            full_response += ai_message_chunk.content
                            message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

                # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
                st.session_state.conversation_manager.add_conversation_pair(
                    user_message, AIMessage(content=full_response)
                )

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
                st.session_state.messages.append(
                    {"role": "assistant", "content": full_response}
                )

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.info("OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
